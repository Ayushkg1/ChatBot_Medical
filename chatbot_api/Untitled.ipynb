{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30be007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "#import gensim\n",
    "#from gensim.summarization import summarize\n",
    "#for importing and manuplating data\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import string\n",
    "import csv\n",
    "from googlesearch import search\n",
    "#for fitting model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "#connecting with google, request data and apllying nlp\n",
    "import requests\n",
    "from lxml import html\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import urllib.request \n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "import requests\n",
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "\n",
    "# Create your views here.\n",
    "\n",
    "\n",
    "def tt(input_query):\n",
    "  USE_CUDA = torch.cuda.is_available()\n",
    "  device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "  def listToString(s):\n",
    "    str1 = \"\"  \n",
    "    for ele in s: \n",
    "        str1 += ele + \" \"\n",
    "    return str1 \n",
    "  def cleaner(x):\n",
    "    return [a for a in (''.join([a for a in x if a not in string.punctuation])).lower().split()]\n",
    "  \n",
    "  csv_file = \"Classification_dataset_cureya.csv\"\n",
    "  df = pd.read_csv(csv_file)\n",
    "  Pipe = Pipeline([\n",
    "      ('bow',CountVectorizer(analyzer=cleaner)),\n",
    "      ('tfidf',TfidfTransformer()),\n",
    "      ('classifier',DecisionTreeClassifier())\n",
    "  ])\n",
    "  Pipe.fit(df['patterns'],df['tag'])\n",
    "\n",
    "  def spacy_process_i(text):\n",
    "    doc = nlp(text)\n",
    "    lemma_list = []\n",
    "    for token in doc:\n",
    "        lemma_list.append(token.lemma_)\n",
    "    filtered_sentence =[] \n",
    "    for word in lemma_list:\n",
    "        lexeme = nlp.vocab[word]\n",
    "        if lexeme.is_stop == False:\n",
    "            filtered_sentence.append(word) \n",
    "    \n",
    "    punctuations=\"?:!.,;\"\n",
    "    for word in filtered_sentence:\n",
    "        if word in punctuations:\n",
    "            filtered_sentence.remove(word)\n",
    "    \n",
    "    counter_string = listToString(filtered_sentence)\n",
    "    pp_string = counter_string.replace(\"-PRON-\", \"\")\n",
    "    \n",
    "    return pp_string\n",
    "  \n",
    "  def loadLines(fileName, fields):\n",
    "      lines = {}\n",
    "      with open(fileName, 'r', encoding='iso-8859-1') as f:\n",
    "          for line in f:\n",
    "              values = line.split(\" +++$+++ \")\n",
    "              lineObj = {}\n",
    "              for i, field in enumerate(fields):\n",
    "                  lineObj[field] = values[i]\n",
    "              lines[lineObj['lineID']] = lineObj\n",
    "      return lines\n",
    "\n",
    "\n",
    "  def loadConversations(fileName, lines, fields):\n",
    "      conversations = []\n",
    "      with open(fileName, 'r', encoding='iso-8859-1') as f:\n",
    "          for line in f:\n",
    "              values = line.split(\" +++$+++ \")\n",
    "              convObj = {}\n",
    "              for i, field in enumerate(fields):\n",
    "                  convObj[field] = values[i]\n",
    "\n",
    "              utterance_id_pattern = re.compile('L[0-9]+')\n",
    "              lineIds = utterance_id_pattern.findall(convObj[\"utteranceIDs\"])\n",
    "              convObj[\"lines\"] = []\n",
    "              for lineId in lineIds:\n",
    "                  convObj[\"lines\"].append(lines[lineId])\n",
    "              conversations.append(convObj)\n",
    "      return conversations\n",
    "\n",
    "\n",
    "  def extractSentencePairs(conversations):\n",
    "      qa_pairs = []\n",
    "      for conversation in conversations:\n",
    "          for i in range(len(conversation[\"lines\"]) - 1): \n",
    "              inputLine = conversation[\"lines\"][i][\"text\"].strip()\n",
    "              targetLine = conversation[\"lines\"][i+1][\"text\"].strip()\n",
    "              if inputLine and targetLine:\n",
    "                  qa_pairs.append([inputLine, targetLine])\n",
    "      return qa_pairs\n",
    "\n",
    "\n",
    "  PAD_token = 0  \n",
    "  SOS_token = 1  \n",
    "  EOS_token = 2 \n",
    "\n",
    "  class Voc:\n",
    "      def __init__(self, name):\n",
    "          self.name = name\n",
    "          self.trimmed = False\n",
    "          self.word2index = {}\n",
    "          self.word2count = {}\n",
    "          self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "          self.num_words = 3  \n",
    "\n",
    "      def addSentence(self, sentence):\n",
    "          for word in sentence.split(' '):\n",
    "              self.addWord(word)\n",
    "\n",
    "      def addWord(self, word):\n",
    "          if word not in self.word2index:\n",
    "              self.word2index[word] = self.num_words\n",
    "              self.word2count[word] = 1\n",
    "              self.index2word[self.num_words] = word\n",
    "              self.num_words += 1\n",
    "          else:\n",
    "              self.word2count[word] += 1\n",
    "\n",
    "      def trim(self, min_count):\n",
    "          if self.trimmed:\n",
    "              return\n",
    "          self.trimmed = True\n",
    "\n",
    "          keep_words = []\n",
    "\n",
    "          for k, v in self.word2count.items():\n",
    "              if v >= min_count:\n",
    "                  keep_words.append(k)\n",
    "\n",
    "          print('keep_words {} / {} = {:.4f}'.format(\n",
    "              len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "          ))\n",
    "\n",
    "          \n",
    "          self.word2index = {}\n",
    "          self.word2count = {}\n",
    "          self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "          self.num_words = 3 \n",
    "\n",
    "          for word in keep_words:\n",
    "              self.addWord(word)\n",
    "\n",
    "\n",
    "  MAX_LENGTH = 10\n",
    "  def unicodeToAscii(s):\n",
    "      return ''.join(\n",
    "          c for c in unicodedata.normalize('NFD', s)\n",
    "          if unicodedata.category(c) != 'Mn'\n",
    "      )\n",
    "\n",
    "  def normalizeString(s):\n",
    "      s = unicodeToAscii(s.lower().strip())\n",
    "      s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "      s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "      s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "      return s\n",
    "\n",
    "  def readVocs(datafile, corpus_name):\n",
    "      print(\"Reading lines...\")\n",
    "      lines = open(datafile, encoding='utf-8').\\\n",
    "          read().strip().split('\\n')\n",
    "      pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "      voc = Voc(corpus_name)\n",
    "      return voc, pairs\n",
    "\n",
    "  def filterPair(p):\n",
    "      return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "\n",
    "  def filterPairs(pairs):\n",
    "      return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "  def loadPrepareData(corpus, corpus_name, datafile, save_dir):\n",
    "      print(\"Start preparing training data ...\")\n",
    "      voc, pairs = readVocs(datafile, corpus_name)\n",
    "      print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
    "      pairs = filterPairs(pairs)\n",
    "      print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
    "      print(\"Counting words...\")\n",
    "      for pair in pairs:\n",
    "          voc.addSentence(pair[0])\n",
    "          voc.addSentence(pair[1])\n",
    "      print(\"Counted words:\", voc.num_words)\n",
    "      return voc, pairs\n",
    "\n",
    "\n",
    "  MIN_COUNT = 3   \n",
    "\n",
    "  def trimRareWords(voc, pairs, MIN_COUNT):\n",
    "      voc.trim(MIN_COUNT)\n",
    "      keep_pairs = []\n",
    "      for pair in pairs:\n",
    "          input_sentence = pair[0]\n",
    "          output_sentence = pair[1]\n",
    "          keep_input = True\n",
    "          keep_output = True\n",
    "          for word in input_sentence.split(' '):\n",
    "              if word not in voc.word2index:\n",
    "                  keep_input = False\n",
    "                  break\n",
    "          for word in output_sentence.split(' '):\n",
    "              if word not in voc.word2index:\n",
    "                  keep_output = False\n",
    "                  break\n",
    "\n",
    "          if keep_input and keep_output:\n",
    "              keep_pairs.append(pair)\n",
    "\n",
    "      print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
    "      return keep_pairs\n",
    "\n",
    "  def indexesFromSentence(voc, sentence):\n",
    "      return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "\n",
    "\n",
    "  def zeroPadding(l, fillvalue=PAD_token):\n",
    "      return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "  def binaryMatrix(l, value=PAD_token):\n",
    "      m = []\n",
    "      for i, seq in enumerate(l):\n",
    "          m.append([])\n",
    "          for token in seq:\n",
    "              if token == PAD_token:\n",
    "                  m[i].append(0)\n",
    "              else:\n",
    "                  m[i].append(1)\n",
    "      return m\n",
    "\n",
    "  def inputVar(l, voc):\n",
    "      indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "      lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "      padList = zeroPadding(indexes_batch)\n",
    "      padVar = torch.LongTensor(padList)\n",
    "      return padVar, lengths\n",
    "\n",
    "  def outputVar(l, voc):\n",
    "      indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "      max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "      padList = zeroPadding(indexes_batch)\n",
    "      mask = binaryMatrix(padList)\n",
    "      mask = torch.BoolTensor(mask)\n",
    "      padVar = torch.LongTensor(padList)\n",
    "      return padVar, mask, max_target_len\n",
    "\n",
    "  def batch2TrainData(voc, pair_batch):\n",
    "      pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "      input_batch, output_batch = [], []\n",
    "      for pair in pair_batch:\n",
    "          input_batch.append(pair[0])\n",
    "          output_batch.append(pair[1])\n",
    "      inp, lengths = inputVar(input_batch, voc)\n",
    "      output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "      return inp, lengths, output, mask, max_target_len\n",
    "\n",
    "\n",
    "  class EncoderRNN(nn.Module):\n",
    "      def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "          super(EncoderRNN, self).__init__()\n",
    "          self.n_layers = n_layers\n",
    "          self.hidden_size = hidden_size\n",
    "          self.embedding = embedding\n",
    "          self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                            dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "      def forward(self, input_seq, input_lengths, hidden=None):\n",
    "          embedded = self.embedding(input_seq)\n",
    "          packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "          outputs, hidden = self.gru(packed, hidden)\n",
    "          outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "          \n",
    "          outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "      \n",
    "          return outputs, hidden\n",
    "\n",
    "\n",
    "  class Attn(nn.Module):\n",
    "      def __init__(self, method, hidden_size):\n",
    "          super(Attn, self).__init__()\n",
    "          self.method = method\n",
    "          if self.method not in ['dot', 'general', 'concat']:\n",
    "              raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "          self.hidden_size = hidden_size\n",
    "          if self.method == 'general':\n",
    "              self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "          elif self.method == 'concat':\n",
    "              self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "              self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "      def dot_score(self, hidden, encoder_output):\n",
    "          return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "      def general_score(self, hidden, encoder_output):\n",
    "          energy = self.attn(encoder_output)\n",
    "          return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "      def concat_score(self, hidden, encoder_output):\n",
    "          energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "          return torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "      def forward(self, hidden, encoder_outputs):\n",
    "          if self.method == 'general':\n",
    "              attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "          elif self.method == 'concat':\n",
    "              attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "          elif self.method == 'dot':\n",
    "              attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "\n",
    "          attn_energies = attn_energies.t()\n",
    "\n",
    "\n",
    "          return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
    "\n",
    "\n",
    "\n",
    "  class LuongAttnDecoderRNN(nn.Module):\n",
    "      def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "          super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "          self.attn_model = attn_model\n",
    "          self.hidden_size = hidden_size\n",
    "          self.output_size = output_size\n",
    "          self.n_layers = n_layers\n",
    "          self.dropout = dropout\n",
    "\n",
    "          self.embedding = embedding\n",
    "          self.embedding_dropout = nn.Dropout(dropout)\n",
    "          self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "          self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "          self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "          self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "      def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "          embedded = self.embedding(input_step)\n",
    "          embedded = self.embedding_dropout(embedded)\n",
    "          rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "          attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "          context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "          rnn_output = rnn_output.squeeze(0)\n",
    "          context = context.squeeze(1)\n",
    "          concat_input = torch.cat((rnn_output, context), 1)\n",
    "          concat_output = torch.tanh(self.concat(concat_input))\n",
    "          output = self.out(concat_output)\n",
    "          output = F.softmax(output, dim=1)\n",
    "          return output, hidden\n",
    "\n",
    "\n",
    "  def maskNLLLoss(inp, target, mask):\n",
    "      nTotal = mask.sum()\n",
    "      crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "      loss = crossEntropy.masked_select(mask).mean()\n",
    "      loss = loss.to(device)\n",
    "      return loss, nTotal.item()\n",
    "\n",
    "\n",
    "  def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
    "            encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
    "\n",
    "      encoder_optimizer.zero_grad()\n",
    "      decoder_optimizer.zero_grad()\n",
    "\n",
    "      input_variable = input_variable.to(device)\n",
    "      target_variable = target_variable.to(device)\n",
    "      mask = mask.to(device)\n",
    "      lengths = lengths.to(\"cpu\")\n",
    "\n",
    "      loss = 0\n",
    "      print_losses = []\n",
    "      n_totals = 0\n",
    "\n",
    "      encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "      decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "      decoder_input = decoder_input.to(device)\n",
    "\n",
    "      decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "      use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "      if use_teacher_forcing:\n",
    "          for t in range(max_target_len):\n",
    "              decoder_output, decoder_hidden = decoder(\n",
    "                  decoder_input, decoder_hidden, encoder_outputs\n",
    "              )\n",
    "              decoder_input = target_variable[t].view(1, -1)\n",
    "              mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "              loss += mask_loss\n",
    "              print_losses.append(mask_loss.item() * nTotal)\n",
    "              n_totals += nTotal\n",
    "      else:\n",
    "          for t in range(max_target_len):\n",
    "              decoder_output, decoder_hidden = decoder(\n",
    "                  decoder_input, decoder_hidden, encoder_outputs\n",
    "              )\n",
    "              \n",
    "              _, topi = decoder_output.topk(1)\n",
    "              decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "              decoder_input = decoder_input.to(device)\n",
    "              \n",
    "              mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "              loss += mask_loss\n",
    "              print_losses.append(mask_loss.item() * nTotal)\n",
    "              n_totals += nTotal\n",
    "\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "      _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "      encoder_optimizer.step()\n",
    "      decoder_optimizer.step()\n",
    "\n",
    "      return sum(print_losses) / n_totals\n",
    "\n",
    "\n",
    "\n",
    "  def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
    "\n",
    "      training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                        for _ in range(n_iteration)]\n",
    "\n",
    "      print('Initializing ...')\n",
    "      start_iteration = 1\n",
    "      print_loss = 0\n",
    "      if loadFilename:\n",
    "          start_iteration = checkpoint['iteration'] + 1\n",
    "\n",
    "      print(\"Training...\")\n",
    "      for iteration in range(start_iteration, n_iteration + 1):\n",
    "          training_batch = training_batches[iteration - 1]\n",
    "          input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "          loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
    "                      decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "          print_loss += loss\n",
    "\n",
    "          if iteration % print_every == 0:\n",
    "              print_loss_avg = print_loss / print_every\n",
    "              print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "              print_loss = 0\n",
    "\n",
    "          if (iteration % save_every == 0):\n",
    "              directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "              if not os.path.exists(directory):\n",
    "                  os.makedirs(directory)\n",
    "              torch.save({\n",
    "                  'iteration': iteration,\n",
    "                  'en': encoder.state_dict(),\n",
    "                  'de': decoder.state_dict(),\n",
    "                  'en_opt': encoder_optimizer.state_dict(),\n",
    "                  'de_opt': decoder_optimizer.state_dict(),\n",
    "                  'loss': loss,\n",
    "                  'voc_dict': voc.__dict__,\n",
    "                  'embedding': embedding.state_dict()\n",
    "              }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))\n",
    "\n",
    "\n",
    "  class GreedySearchDecoder(nn.Module):\n",
    "      def __init__(self, encoder, decoder):\n",
    "          super(GreedySearchDecoder, self).__init__()\n",
    "          self.encoder = encoder\n",
    "          self.decoder = decoder\n",
    "\n",
    "      def forward(self, input_seq, input_length, max_length):\n",
    "          encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "          decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "          decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "          all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "          all_scores = torch.zeros([0], device=device)\n",
    "          for _ in range(max_length):\n",
    "              decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "              decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "              all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "              all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "              decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "          return all_tokens, all_scores\n",
    "\n",
    "\n",
    "  def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "      indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "      lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "      input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "      input_batch = input_batch.to(device)\n",
    "      lengths = lengths.to(\"cpu\")\n",
    "      tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "      decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "      return decoded_words\n",
    "\n",
    "  def evaluateInput(input_s,encoder, decoder, searcher, voc):\n",
    "    input_sentence = ''\n",
    "    try:\n",
    "      input_sentence = input_s\n",
    "      if input_sentence == 'q' or input_sentence == 'quit':\n",
    "        return(\"End\")\n",
    "      input_sentence = normalizeString(input_sentence)\n",
    "      output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "      output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "      return listToString(output_words)\n",
    "\n",
    "    except KeyError:\n",
    "      return(\"Error: Encountered unknown word.\")\n",
    "\n",
    "\n",
    "  corpus_name = \"cornell movie-dialogs corpus\"\n",
    "  corpus = \"p/data\"\n",
    "\n",
    "  def printLines(file, n=10):\n",
    "      with open(file, 'rb') as datafile:\n",
    "          lines = datafile.readlines()\n",
    "      for line in lines[:n]:\n",
    "          print(line)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  datafile = os.path.join(corpus, \"formatted_movie_lines.txt\")\n",
    "  save_dir = os.path.join(\"data\", \"save\")\n",
    "  voc, pairs = loadPrepareData(corpus, corpus_name, datafile, save_dir)  \n",
    "  model_name = 'cb_model'\n",
    "  attn_model = 'dot'\n",
    "  hidden_size = 500\n",
    "  encoder_n_layers = 2\n",
    "  decoder_n_layers = 2\n",
    "  dropout = 0.1\n",
    "  batch_size = 64\n",
    "\n",
    "  loadFilename = \"4000_checkpoint.tar\"\n",
    "  checkpoint_iter = 4000\n",
    "\n",
    "\n",
    "  if loadFilename:\n",
    "      checkpoint = torch.load(loadFilename)\n",
    "      encoder_sd = checkpoint['en']\n",
    "      decoder_sd = checkpoint['de']\n",
    "      encoder_optimizer_sd = checkpoint['en_opt']\n",
    "      decoder_optimizer_sd = checkpoint['de_opt']\n",
    "      embedding_sd = checkpoint['embedding']\n",
    "      voc.__dict__ = checkpoint['voc_dict']\n",
    "\n",
    "\n",
    "  print('Building encoder and decoder ...')\n",
    "  embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "  if loadFilename:\n",
    "      embedding.load_state_dict(embedding_sd)\n",
    "  encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "  decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "  if loadFilename:\n",
    "      encoder.load_state_dict(encoder_sd)\n",
    "      decoder.load_state_dict(decoder_sd)\n",
    "  encoder = encoder.to(device)\n",
    "  decoder = decoder.to(device)\n",
    "  print('Models built and ready to go!')\n",
    "\n",
    "  encoder.eval()\n",
    "  decoder.eval()\n",
    "\n",
    "  searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "  def search_web(query):\n",
    "    search_result_list1 = []\n",
    "    search_result_list2 = []\n",
    "    search_result_list3 = []\n",
    "    disease = query\n",
    "    website = ['mayo clinic', 'apollo clinic','cdc']\n",
    "    query1 =disease + \" \" + website[0] \n",
    "    #print(query1)\n",
    "    search_result_list1.append(list(search(query1, tld=\"co.in\", num=3, stop=5, pause=2)))\n",
    "    #print(search_result_list1)\n",
    "    query2 = disease + \" \" + website[1] \n",
    "    #print(query2)\n",
    "    search_result_list2.append(list(search(query2, tld=\"co.in\", num=3, stop=5, pause=2)))\n",
    "    #print(search_result_list2)\n",
    "    query3 = disease + \" \" + website[2] \n",
    "    #print(query3)\n",
    "    search_result_list3.append(list(search(query3, tld=\"co.in\", num=3, stop=5, pause=2)))\n",
    "    #print(search_result_list3)\n",
    "\n",
    "    return search_result_list1,search_result_list2,search_result_list3\n",
    "\n",
    "  def mayo_info(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    list1 = []\n",
    "    for header in soup.find_all('p'):\n",
    "        nextNode = header\n",
    "        if(True):\n",
    "          #print(\"heaad\"+header.text.strip())\n",
    "          list1.append(header.text.strip())\n",
    "          while True:\n",
    "              nextNode = nextNode.nextSibling\n",
    "              if nextNode is None:\n",
    "                  break\n",
    "              if isinstance(nextNode, NavigableString):\n",
    "                  #list1.append(nextNode.strip())\n",
    "                  print(\"\")\n",
    "                  #print(\"bye\")\n",
    "                  #print(nextNode.strip())\n",
    "              if isinstance(nextNode, Tag):\n",
    "                  if nextNode.name == \"p\":\n",
    "                      break\n",
    "                  #print (nextNode.get_text(strip=True).strip())\n",
    "                  #print(list1)\n",
    "                  #print(\"hei\")\n",
    "                  #list1.append(nextNode.get_text(strip=True).strip())\n",
    "\n",
    "    #print(list1)\n",
    "    reference = listToString(list1)\n",
    "    reference = reference.replace(' and the triple-shield Mayo Clinic logo are trademarks of Mayo Foundation for Medical Education and Research.', '')\n",
    "    reference = reference.replace('Check out these best-sellers and special offers on books and newsletters from Mayo Clinic Press.', '')\n",
    "    reference = reference.replace('Mayo Clinic does not endorse companies or products. Advertising revenue supports our not-for-profit mission.', '')\n",
    "    reference = reference.replace('We are open for safe in-person care.', '')\n",
    "    reference = reference.replace('Featured conditions', '')\n",
    "    reference = reference.replace('Any use of this site constitutes your agreement to the Terms and Conditions and Privacy Policy linked below.\\n\\nTerms and Conditions\\nPrivacy Policy\\nNotice of Privacy Practices\\nNotice of Nondiscrimination\\nManage Cookies', '')\n",
    "    reference = reference.replace('MayoClinic.org', '')\n",
    "    reference = reference.replace('Mayo Foundation ', '')\n",
    "    reference = reference.replace('This site complies with the  HONcode standard for trustworthy health information: verify here.', '')\n",
    "    reference = reference.lower()\n",
    "    reference = reference.replace('mayo', '')\n",
    "    reference = reference.replace('clinic', '')\n",
    "    reference = reference.replace('\",\"', '')\n",
    "    reference = reference.replace('a single copy of these materials may be reprinted for noncommercial personal use only.', '')\n",
    "    reference = reference.replace('  ', '')\n",
    "    reference = reference.replace('\"', '')\n",
    "    #print(reference)\n",
    "    flag = 0\n",
    "    sol = []\n",
    "    # traverse paragraphs from soup\n",
    "    for data in soup.find_all(\"h2\"):\n",
    "        if(data.text.strip() == 'Prevention' or data.text.strip() == 'Overview' or data.text.strip() == 'Symptoms' or data.text.strip() == 'Causes' or data.text.strip() == 'Complications' or data.text.strip() == 'Risk factors' or data.text.strip() == 'Diagnosis' or data.text.strip() == 'Treatment'):\n",
    "          sol.append(data.text.strip())\n",
    "          sol.append(\"\\n\")\n",
    "          #print(data.text.strip())\n",
    "          para = data.find_next_sibling('ul')\n",
    "          sol.append(para.text)\n",
    "          #print(para.text)\n",
    "          flag = 1\n",
    "\n",
    "    if flag == 0:\n",
    "      resultant = soup.findAll('h2')\n",
    "      for i in range(len(resultant)):\n",
    "        sol.append(resultant[i].text)\n",
    "\n",
    "      #print(sol)\n",
    "\n",
    "    sol = list(dict.fromkeys(sol))\n",
    "    reference_counter = listToString(sol)\n",
    "\n",
    "    #print(\"\\nReference: \", *search_result_list)\n",
    "    return reference,reference_counter\n",
    "\n",
    "\n",
    "  def cdc_info(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    list = []\n",
    "    for header in soup.find_all('p'):\n",
    "        nextNode = header\n",
    "        if(True):\n",
    "          #print(\"heaad\"+header.text.strip())\n",
    "          list.append(header.text.strip())\n",
    "          while True:\n",
    "              nextNode = nextNode.nextSibling\n",
    "              if nextNode is None:\n",
    "                  break\n",
    "              if isinstance(nextNode, NavigableString):\n",
    "                print(\"\")\n",
    "                #list.append(nextNode.strip())\n",
    "              if isinstance(nextNode, Tag):\n",
    "                  if nextNode.name == \"p\":\n",
    "                      break\n",
    "                  #print (nextNode.get_text(strip=True).strip())\n",
    "                  #print(list)\n",
    "                  #print(\"hi\")\n",
    "                  #list.append(nextNode.get_text(strip=True).strip())\n",
    "\n",
    "    reference = listToString(list)\n",
    "    reference = reference.replace('Your email address will not be published. Required fields are marked', '')\n",
    "    reference = reference.replace('Comment', '')\n",
    "    reference = reference.replace('Name', '')\n",
    "    reference = reference.replace('Website', '')\n",
    "    reference = reference.replace('*', '')\n",
    "    reference = reference.lower()\n",
    "    reference = reference.replace('all rights reserved', '')\n",
    "    #print(reference)\n",
    "    return reference\n",
    "\n",
    "\n",
    "  def apollo_info(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    list = []\n",
    "    for header in soup.find_all('p'):\n",
    "        nextNode = header\n",
    "        if(True):\n",
    "          #print(\"heaad\"+header.text.strip())\n",
    "          list.append(header.text.strip())\n",
    "          while True:\n",
    "              nextNode = nextNode.nextSibling\n",
    "              if nextNode is None:\n",
    "                  break\n",
    "              if isinstance(nextNode, NavigableString):\n",
    "                print(\"\")\n",
    "                #list.append(nextNode.strip())\n",
    "              if isinstance(nextNode, Tag):\n",
    "                  if nextNode.name == \"p\":\n",
    "                      break\n",
    "                  #print (nextNode.get_text(strip=True).strip())\n",
    "                  #print(list)\n",
    "                  #print(\"hi\")\n",
    "                  #list.append(nextNode.get_text(strip=True).strip())\n",
    "\n",
    "    reference = listToString(list)\n",
    "    reference = reference.replace('Your email address will not be published. Required fields are marked', '')\n",
    "    reference = reference.replace('Comment', '')\n",
    "    reference = reference.replace('Name', '')\n",
    "    reference = reference.replace('Website', '')\n",
    "    reference = reference.replace('*', '')\n",
    "    reference = reference.lower()\n",
    "    reference = reference.replace('all rights reserved', '')\n",
    "    reference = reference.replace('apollo hospitals enterprise limited', '')\n",
    "    reference = reference.replace('apollo hospitals', '')\n",
    "    reference = reference.replace('https://www.apolloclinic.com/clinic-locator', '')\n",
    "    reference = reference.replace('email', '')\n",
    "    #print(reference)\n",
    "    return reference\n",
    "\n",
    "  def get_context(search_result_list1,search_result_list2,search_result_list3):\n",
    "    url = search_result_list1[0]\n",
    "    r = requests.get(url[0])\n",
    "    html = r.text\n",
    "    context_mayo,context_mayo2 = mayo_info(html)\n",
    "    url2 = search_result_list2[0]\n",
    "    r2 = requests.get(url2[0])\n",
    "    html2 = r2.text\n",
    "    context_apollo = apollo_info(html2)\n",
    "    url3 = search_result_list3[0]\n",
    "    r3 = requests.get(url3[0])\n",
    "    html3 = r3.text\n",
    "    context_cdc = cdc_info(html3)\n",
    "    return context_apollo,context_mayo,context_mayo2,context_cdc\n",
    "\n",
    "  nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "  def spacy_process(text):\n",
    "    doc = nlp(text)\n",
    "    lemma_list = []\n",
    "    for token in doc:\n",
    "        lemma_list.append(token.lemma_)\n",
    "    #print(\"Tokenize+Lemmatize:\")\n",
    "    #print(lemma_list)\n",
    "    \n",
    "    #removes stop-word like at , i , am ,etc\n",
    "\n",
    "    #Filter the stopword\n",
    "    #filtered_sentence =[] \n",
    "    #for word in lemma_list:\n",
    "        #lexeme = nlp.vocab[word]\n",
    "        #if lexeme.is_stop == False:\n",
    "            #filtered_sentence.append(word) \n",
    "    \n",
    "    #Remove punctuation\n",
    "    #punctuations=\"?:!.,;\"\n",
    "    #for word in filtered_sentence:\n",
    "        #if word in punctuations:\n",
    "            #filtered_sentence.remove(word)\n",
    "    \n",
    "    counter_string = listToString(lemma_list)\n",
    "    pp_string = counter_string.replace(\"-PRON-\", \"\")\n",
    "    \n",
    "    return pp_string\n",
    "\n",
    "\n",
    "  def qadiseasebert(question,text):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"biobert_v1.1_pubmed_squad_v2\",model_max_length=512)\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(\"biobert_v1.1_pubmed_squad_v2\")\n",
    "    inputs = tokenizer(question, text, add_special_tokens=True, truncation=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "    outputs = model(**inputs)\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "    #print(f\"Question: {question}\")\n",
    "    answer = answer.replace(\"[CLS]\",\"\")\n",
    "    answer = answer.replace(\"[SEP]\",\"\")\n",
    "    #print(answer)\n",
    "    answer = answer.replace(question,\"\")\n",
    "    answer = answer.replace(spacy_process(question),\"\")\n",
    "    #print(answer)\n",
    "    #print(f\"Answer: {answer}\")\n",
    "    return answer\n",
    "\n",
    "  def txt_pp(question,text):\n",
    "    counter = spacy_process(question)\n",
    "    res = text.split()\n",
    "    #print(len(res))\n",
    "    '''\n",
    "    if(len(res)>400):\n",
    "      text = spacy_process(text)\n",
    "      res = text.split()\n",
    "      #print(len(res))\n",
    "      if(len(res)>300):\n",
    "        text=summarize(text,word_count=400)\n",
    "    text = counter+\" \"+text\n",
    "    '''\n",
    "    return question,text\n",
    "\n",
    "  def qadisease_c(query):\n",
    "    list1,list2,list3 = search_web(query)\n",
    "    context1,context2,context2_1,context3 = get_context(list1,list2,list3)\n",
    "    _,ctxt1=txt_pp(query,context1)\n",
    "    _,ctxt2=txt_pp(query,context2)\n",
    "    _,ctxt3=txt_pp(query,context2_1)\n",
    "    _,ctxt4=txt_pp(query,context3)\n",
    "    ans1 = qadiseasebert(query,ctxt1)\n",
    "    ans2 = qadiseasebert(query,ctxt2)\n",
    "    ans3 = qadiseasebert(query,ctxt3)\n",
    "    ans4 = qadiseasebert(query,ctxt4)\n",
    "    score = []\n",
    "    ans_t = []\n",
    "    counter_q = nlp(spacy_process(query))\n",
    "    score.append(counter_q.similarity(nlp(ans1)))\n",
    "    score.append(counter_q.similarity(nlp(ans2)))\n",
    "    score.append(counter_q.similarity(nlp(ans3)))\n",
    "    score.append(counter_q.similarity(nlp(ans4)))\n",
    "    ans_t.append(ans1)\n",
    "    ans_t.append(ans2)\n",
    "    ans_t.append(ans3)\n",
    "    ans_t.append(ans4)\n",
    "    print(score)\n",
    "    return score,ans_t\n",
    "\n",
    "  def f_ans(score,ans):\n",
    "    fallback = \"Couldnt understand your query, Please try again\"\n",
    "    maxpos = score.index(max(score))\n",
    "    if maxpos == 0:\n",
    "      return ans[0]\n",
    "    elif maxpos == 1:\n",
    "      return ans[1]\n",
    "    elif maxpos == 2:\n",
    "      return ans[2]\n",
    "    elif maxpos == 3:\n",
    "      return ans[3]\n",
    "    else:\n",
    "      return fallback\n",
    "\n",
    "  def reemovNestings(l): \n",
    "      for i in l:\n",
    "          if type(i) == list:\n",
    "              reemovNestings(i)\n",
    "          else:\n",
    "              output.append(i)\n",
    "\n",
    "  def servicecard(lst):\n",
    "    lst_p = []\n",
    "    def reemovNestings(l): \n",
    "      for i in l:\n",
    "          if type(i) == list:\n",
    "              reemovNestings(i)\n",
    "          else:\n",
    "              lst_p.append(i)\n",
    "    reemovNestings(lst)\n",
    "    keywords_check = [\"yoga\",\"excercise\",\"diet\",\"nutrition\",\"nutritional\",\"nutritionist\",\"dietician\",\"consultation\",\"treatment\",\"alternative treatment\",\"weight management\",\"muscle gain\",\"pcod\",\"cardiovascular disease\",\"renal\",\"anaemia\",\"gastrointestinal\",\"disease\",\"ayurveda\",\"ayurvedic\",\"naturopathy\",\"homeopathy\",\"unani\",\"siddha\",\"cureya\",\"doctor\",\"blood pressure\",\"heart\",\"artery\",\"overweight\",\"underweight\",\"pcos\",\"thyroid\",\"cardiovascular\",\"dialysis\",\"typhoid\",\"influenza\",\"malaria\",\"aids\"]\n",
    "    set_a = set(lst_p)\n",
    "    set_b = set(keywords_check)\n",
    "    set_c = set_a & set_b\n",
    "    output = list(set_c)\n",
    "    output = listToString(output)\n",
    "    return output\n",
    "\n",
    "  counter = 1\n",
    "  msg_type = 0\n",
    "  msg_track = []\n",
    "  url1 = \"\"\n",
    "  url2 = \"\"\n",
    "  price = \"\"\n",
    "  while(counter == 1):\n",
    "    query = input_query\n",
    "    output = query\n",
    "    if(output == \"q\" or output == \"quit\" or output == \"end\" or output == \"exit\"):\n",
    "      counter = 0\n",
    "      print(\"Thanks\")\n",
    "      break\n",
    "\n",
    "    output = Pipe.predict([spacy_process_i(query)])[0]\n",
    "    #print(output)\n",
    "    msg_track.append(spacy_process_i(query).split(\" \"))\n",
    "    if(len(msg_track)<3):\n",
    "      if(output == \"medical\"):\n",
    "        msg_type = 1\n",
    "        op,at = qadisease_c(query)\n",
    "        answer = f_ans(op,at)\n",
    "        print(answer)\n",
    "        #print(msg_type)\n",
    "        return msg_type,answer,url1,url2,price \n",
    "      elif(output == \"general\"):\n",
    "        msg_type = 1\n",
    "        answer = evaluateInput(query,encoder, decoder, searcher, voc)\n",
    "        print(answer)\n",
    "        #print(msg_type)\n",
    "        return msg_type,answer,url1,url2,price \n",
    "      elif(output == \"service\"):\n",
    "        msg_type = 2\n",
    "        answer = \"service\"\n",
    "        url1 = \"req1\"\n",
    "        url2 = \"req2\"\n",
    "        price = 450\n",
    "        print(answer)\n",
    "        return msg_type,answer,url1,url2,price \n",
    "    else:\n",
    "      msg_type = 2\n",
    "      answer = servicecard(msg_track)\n",
    "      #print(msg_type)\n",
    "      print(\"\\nCARD-services\\n\")\n",
    "      print(answer)\n",
    "      answer = \"service\"\n",
    "      url1 = \"req1\"\n",
    "      url2 = \"req2\"\n",
    "      price = 450\n",
    "      return msg_type,answer,url1,url2,price \n",
    "\n",
    "  return msg_type,answer,url1,url2,price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8533be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 221282 sentence pairs\n",
      "Trimmed to 64271 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 18008\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[0.45170933525764906, 0.0, 0.0, 0.0]\n",
      "   pre - diabetes, as the name suggests occurs before the diabetes phase and has the potential to evolve into a fully fledged type 2 diabetes down the road. the most frightening aspect of pre - diabetes is that it has no symptoms unless a manual blood test is done. during this condition, the body functions normally, just that the glucose level in the blood is slightly elevated but not to the extent of the diabetes. pre - diabetes open an unpleasant door that may also lead to heart diseases or strokes. but here is a good news, you can prevent pre - diabetes from developing into type 2 disease\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prati\\AppData\\Local\\Temp/ipykernel_10844/3065070839.py:867: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  score.append(counter_q.similarity(nlp(ans1)))\n",
      "C:\\Users\\prati\\AppData\\Local\\Temp/ipykernel_10844/3065070839.py:868: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  score.append(counter_q.similarity(nlp(ans2)))\n",
      "C:\\Users\\prati\\AppData\\Local\\Temp/ipykernel_10844/3065070839.py:868: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  score.append(counter_q.similarity(nlp(ans2)))\n",
      "C:\\Users\\prati\\AppData\\Local\\Temp/ipykernel_10844/3065070839.py:869: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  score.append(counter_q.similarity(nlp(ans3)))\n",
      "C:\\Users\\prati\\AppData\\Local\\Temp/ipykernel_10844/3065070839.py:869: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  score.append(counter_q.similarity(nlp(ans3)))\n",
      "C:\\Users\\prati\\AppData\\Local\\Temp/ipykernel_10844/3065070839.py:870: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  score.append(counter_q.similarity(nlp(ans4)))\n",
      "C:\\Users\\prati\\AppData\\Local\\Temp/ipykernel_10844/3065070839.py:870: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  score.append(counter_q.similarity(nlp(ans4)))\n"
     ]
    }
   ],
   "source": [
    "q,p,r,s,t = tt(\"prevention of diabetes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08e6cd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls= []\n",
    "ls.append(q)\n",
    "ls.append(p)\n",
    "ls.append(r)\n",
    "ls.append(s)\n",
    "ls.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbdea0b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " '   pre - diabetes, as the name suggests occurs before the diabetes phase and has the potential to evolve into a fully fledged type 2 diabetes down the road. the most frightening aspect of pre - diabetes is that it has no symptoms unless a manual blood test is done. during this condition, the body functions normally, just that the glucose level in the blood is slightly elevated but not to the extent of the diabetes. pre - diabetes open an unpleasant door that may also lead to heart diseases or strokes. but here is a good news, you can prevent pre - diabetes from developing into type 2 disease',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0098fa-ccce-4d5d-94d7-c98be72869e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
